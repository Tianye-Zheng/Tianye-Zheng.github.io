---
layout: post
title:  "机器学习(1)-绪论"
date:   2019-02-21
desc: ""
keywords: "机器学习"
categories: [机器学习]
tags: [机器学习]
icon: icon-html
---
《机器学习》西瓜书！

发现一篇宝藏博文。。 [机器学习中的主要符号（LaTeX)](https://blog.csdn.net/wcs_152/article/details/81182669)

# chap1 绪论
<br />

机器学习研究的主要内容，是关于在计算机上从数据中产生 “模型” 的算法，即 “学习算法” （learning algorithm）。模型泛指从数据中学得的结果，有文献用 “模型” 指全局性结果（例如一棵决策树），而用 “模式” 指局部性结果（例如一条规则）

<br />
## 基本术语：

+ 数据集 data set
+ 示例 instance / 样本 sample
+ 属性 attribute
+ 特征 feature
+ 属性值 attribute value
+ 属性空间 / 样本空间 / 输入空间
+ 维数 dimensionality
+ 学习 learning / 训练 training : 从数据中学得模型的过程
+ 训练数据 训练样本 训练集
+ 假设 hypothesis : 学得模型对应了关于数据的某种潜在的规律
+ 真实 ground-truth : 这种潜在规律自身
+ 学习器 learner : 学习算法在给定数据和参数空间上的实例化
+ 标记 label : 关于示例结果的信息
+ 样例 example : 拥有了标记信息的示例
+ 标记空间 / 输出空间
+ 分类 classification : 预测的是离散值
+ 回归 regression : 预测的是连续值
+ 二分类 正类 反类 多分类
+ 测试 testing : 学得模型后，使用其进行预测的过程
+ 测试样本 预测标记
+ 聚类 clustering : 将训练集中的对象分成若干组
+ 簇 cluster : 每组称为一个簇
+ 监督学习 supervised learning : 分类和回归是其代表
+ 无监督学习 unsupervised learning : 聚类是其代表
+ 泛化能力 generalization : 学得模型适用于新样本的能力
+ 独立同分布 independent and identically distributed, i.d.d : 通常假设样本空间中全体样本服从一个未知分布（distribution） D ，我们获得的每个样本都是独立地从这个分布上采样获得的

<br />
## 假设空间

归纳学习（inductive learning）：“从样例中学习” 显然是一个归纳的过程

概念学习 ：狭义的归纳学习则要求从训练数据中学得概念 (concept)

我们可以把学习过程看作一个在所有假设（hypothesis）组成的空间中进行搜索的过程，搜索的目标是找到与训练集 “匹配”（fit）的假设

版本空间（version space）：可能有多个假设与训练集一致，即存在着一个与训练集一致的 “假设集合”

<br />
## 归纳偏好

归纳偏好 （inductive bias）：机器学习算法在学习过程中对某种类型假设的偏好，简称 “偏好”

“ 奥卡姆剃刀 ” ：若有多个假设与观察一致，则选最简单的那个

“ 没有免费的午餐 ” 定理（No Free Lunch Theorem, NFL定理）：（该定理的理解）对于一个学习算法 $$\mathfrak{L}_{a}$$ ，若它在某些问题上比学习算法 $$\mathfrak{L}_{b}$$ 好，则必然存在另一些问题，在那里 $$\mathfrak{L}_{b}$$ 比 $$\mathfrak{L}_{a}$$ 好。这个结论对任何算法均成立

---

简单说明如下，假设样本空间 $$\mathcal{X}$$ 和假设空间 $$\mathcal{H}$$ 都是离散的。

令 $$P(h\mid X,\mathfrak{L}_{a})$$ 代表算法 $$\mathfrak{L}_{a}$$ 基于训练数据 X 产生假设 h 的概率，

f 代表希望学习的真实目标函数

$$\mathfrak{L}_{a}$$ 的 “训练集外误差”，即 $$\mathfrak{L}_{a}$$ 在训练集之外的所有样本上的误差为

$$E_{ote}(\mathfrak{L}_{a}|X,f)=\Sigma_{h}\Sigma_{x\in \mathcal{X}-X}P(x)\mathbb{I}(h(x)\neq f(x))P(h|X,\mathfrak{L}_{a})$$

其中 $$\mathbb{I}(\cdot)$$ 是指示函数，若 $$\cdot$$ 为真则取值 1 ，否则取值 0

简单起见，现只考虑二分类问题，且真实目标函数可以是任何函数 $$\mathcal{X}\mapsto \{0,1\}$$，函数空间为 $$\{0,1\}^{\mid\mathcal{X}\mid}$$

证明的关键是说明对于任意的学习算法 $$\mathfrak{L}_{a}$$，对所有可能的 f 的累计 “训练集外误差” 与算法本身无关

$$
\Sigma_{f}E_{ote}(\mathfrak{L}_{a}|X,f)
= \Sigma_{f}\Sigma_{h}\Sigma_{x\in \mathcal{X}-X}P(x)\mathbb{I}(h(x)\neq f(x))P(h|X,\mathfrak{L}_{a}) \\
= \Sigma_{x\in \mathcal{X}-X}P(x)\Sigma_{h}P(h|X,\mathfrak{L}_{a})\Sigma_{f}\mathbb{I}(h(x)\neq f(x))  \\
= \frac{1}{2}2^{|\mathcal{X}|}\Sigma_{x\in \mathcal{X}-X}P(x)\Sigma_{h}P(h|X,\mathfrak{L}_{a}) \\
= 2^{|\mathcal{X}|-1}\Sigma_{x\in \mathcal{X}-X}P(x) \cdot 1
$$

注：

若 f 均匀分布，则有一般的 f 对 x 的预测与 h(x) 不一致，故 $$\Sigma_{f}\mathbb{I}(h(x)\neq f(x))=\frac{1}{2}2^{\mid\mathcal{X}\mid}$$

而且很显然地，$$\Sigma_{h}P(h\mid X,\mathfrak{L}_{a})=1$$，这是消去学习算法的 key

结果表明，无论 $$\mathfrak{L}_{a}$$多聪明，$$\mathfrak{L}_{b}$$多笨拙，其期望误差是相同的，即期望性能是相同的