---
layout: post
title:  "机器学习(2)—模型评估与选择"
date:   2019-02-26
desc: ""
keywords: "机器学习"
categories: [机器学习]
tags: [机器学习]
icon: icon-html
---

对应西瓜书第2章内容
<br />
<br />

## 经验误差与过拟合：

学习器在训练集上的误差称为 “ 训练误差 ” 或 “ 经验误差 ” ，在新样本上的误差称为 “ 泛化误差 ”

当学习器把训练样本学得“太好”，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象在机器学习中称为 “ 过拟合 ” （overfitting）

与 “ 过拟合 ” 相对的是 “ 欠拟合 ” ，这是指对训练样本的一般性质尚未学好

通过实验测试来对学习器的泛化误差进行评估并进而做出选择，为此，需要一个 “ 测试集 ” 来测试学习器对新样本的判别能力，然后以测试集上的 “ 测试误差 ” 作为泛化误差的近似

测试集应尽可能与训练集互斥

<br />
## 评估方法：

**留出法 （hold-out）** ：直接将数据集 D 划分为两个互斥的集合，其中一个集合作为训练集 S， 另一个作为测试集 T，即 $$D=S\cup T,S\cap T=\varnothing$$. 在 S 上训练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计

**交叉验证法 （cross validation）** ：先将数据集 D 划分为 k 个大小相似的互斥子集，即

$$D = D_{1}\cup D_{2}\cup ...\cup D_{k},\quad D_{i}\cap D_{j}=\varnothing (i \neq j)$$

每个子集 $$D_{i}$$ 都尽可能保持数据分布的一致性，即从 D 中通过分层采样得到

然后每次用 k - 1 个子集的并集作为训练集，余下的那个子集作为测试集； 这样得到了 k 组训练/测试集，最终返回 k 个测试结果的均值

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/6.png" width = "370" height =
"220"/></div>

通常称为 **k 折交叉验证 k-fold cross validation**

**留一法 （Leave-One-Out）** 交叉验证法中，令 k = m

**自助法 （bootstrapping）** 以自主采样法为基础。给定包含 m 个样本的数据集 D，这样采样产生数据集 $$D^{'}$$ : 每次随机从 D 中挑选一个样本将其拷贝放入 $$D^{'}$$ ，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到。

通过自主采样，D 中约有 36.8% 的样本未出现在 $$D^{'}$$ 中，这样，实际评估的模型与期望评估的模型都使用 m 个训练样本，而仍有数据总量约 1 / 3 的，没在训练集中出现的样本用于测试，这样的测试结果也叫做 “ 包外估计 ” out-of-bag estimate

**参数调节 / 调参** ： 大多数学习算法都有些参数需要设定，参数配置不同，学得模型的性能往往有显著差别

<br />
## 性能度量

性能度量是衡量模型泛化能力的评价标准，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/7.png" width = "440" height =
"240"/></div>

<br />
**分类任务中的性能度量 ：**

### 错误率与精度

对样例集 $$D$$，分类错误率定义为

$$E(f;D)=\frac{1}{m}\sum^{m}_{i=1}\mathbb{I}(f(x_{i})\neq y_{i})$$

精度则定义为

$$E(f;D)=\frac{1}{m}\sum^{m}_{i=1}\mathbb{I}(f(x_{i})= y_{i})=1-E(f;D)$$

更一般的，对于数据分布 $$D$$ 和概率密度函数 $$p(\cdot)$$，错误率和精度分别描述为

$$E(f;D)=\int_{x\sim D}\mathbb{I}(f(x)\neq y)p(x)dx$$

$$acc(f;D)=\int_{x\sim D}\mathbb{I}(f(x)=y)p(x)dx=1-E(f;D)$$

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/8.png" width = "350" height =
"180" /></div>

对于二分类问题，查准率 P 与查全率 R 分别定义为

$$P=\frac{TP}{TP+FP}$$

$$R=\frac{TP}{TP+FN}$$

根据学习器的预测结果对样例进行排序，排在前面的是学习器认为“最可能”是正例的样本，排在最后的则是学习器认为“最不可能”是正例的样本，按此顺序逐个把样本作为正例进行预测，得到 **P-R曲线**

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/9.png" width = "290" height =
"280" /></div>

在比较时，若一个学习器的 P-R 曲线被另一个学习器的曲线完全 “ 包住 ”，则可断言后者的性能优于前者，如果两个学习器的曲线发生交叉，此时一个比较合理的判据是比较曲线下面积的大小

**平衡点 break-event point, BEP** 是一个综合的性能度量，是 查准率 = 查全率 时的取值，图中基于 BEP 比较，可认为学习器 A 优于 B

但 BEP 还是过于简化，更常用的是 **F1 度量**（基于查准率与查全率的调和平均）

$$F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}$$

F1 度量的一般形式 —— $$F_{\beta}$$，能让我们表达出对查准率 / 查全率的不同偏好，定义为

$$F_{\beta}=\frac{(1+\beta^{2})\times P\times R}{(\beta^{2}\times P)+R}$$

*与算术平均和几何平均相比，调和平均更重视较小值*

其中 $$\beta > 0$$ 度量了查全率对查准率的相对重要性，$$\beta = 1$$ 时退化为标准的 F1 ，$$\beta > 1$$ 时查全率有更大影响； $$\beta < 1$$ 时查准率有更大影响

我们希望在 n 个二分类混淆矩阵上综合考察查准率和查全率，一种直接的做法是现在个混淆矩阵上分别计算，记为 $$(P_{1},R_{1})$$，$$(P_{2},R_{2})$$，...，$$(P_{n},R_{n})$$，再计算平均值，这样就得到

$$宏查准率\quad macro-P=\frac{1}{n}\sum^{n}_{i=1}P_{i}$$

$$宏查全率\quad macro-R=\frac{1}{n}\sum^{n}_{i=1}R_{i}$$

$$宏F1\quad macro-F1=\frac{2\times macro-P\times macro-R}{macro-P+macro-R}$$

还可先得到 $$\overline{TP}$$ , $$\overline{FP}$$ , $$\overline{TN}$$ , $$\overline{FN}$$，再基于这些平均值计算出

$$微查准率\quad micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$

$$微查全率\quad micro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$

$$微F1\quad micro-F1=\frac{2\times micro-P\times micro-R}{micro-P+micro-R}$$

与 P-R 曲线类似，根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要量的值，得到 “ ROC曲线 ”

$$真正例率\quad TPR = \frac{TP}{TP+FN}$$

$$假正例率\quad FPR = \frac{FP}{TN+FP}$$

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/10.png" width = "400" height =
"290" /></div>

进行学习器比较时，类似 P-R 图，若一个学习器的 ROC 曲线被另一个学习器的曲线完全 “ 包住 ”，则可断言后者的性能优于前者；若曲线发生交叉，较为合理的判据是比较 ROC 曲线下的面积，即 AUC，假定 ROC 曲线由坐标为 $${(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})}$$，则 AUC 可估算为

$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})\cdot (y_{i}+y_{i+1})$$

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/11.png" width = "520" height =
"280" /></div>

为权衡不同类型错误所造成的不同损失，可为错误赋予 **“ 非均等代价 ” unequal cost**

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/12.png" width = "230" height =
"180" /></div>

其中，$$cost_{ij}$$ 表示将第 i 类样本预测为第 j 类样本的代价；若将第 0 类作为正类，第 1 类作为反类，令 $$D^{+}$$ 与 $$D^{-}$$ 分别代表样例集 $$D$$ 的正例子集和反例子集，则**“ 敏感代价 ”(cost-sensitive) 错误率** 为

$$E(f;D;cost)=\frac{1}{m}\lgroup \sum_{x_{i}\in D^{+}}\mathbb{I}(f(x_{i})\neq y_{i})\times cost_{01}+\sum_{x_{i}\in D^{-}}\mathbb{I}(f(x_{i})\neq y_{i})\times cost_{10}\rgroup$$

“ 代价曲线 ” 能直接反映出学习器的期望总体代价，横轴是取值为 [0,1] 的正例概率代价

$$P(+)cost=\frac{p\times cost_{01}}{p\times cost_{01} + (1-p)\times cost_{10}}$$

其中 p 是样例为正例的概率；纵轴是取值为 [0,1] 的归一化代价

$$cost_{norm}=\frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}$$

其中 FPR 是假正例率，FNR = 1 - TPR 是假反例率

<div align="center"><img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/13.png" width = "280" height =
"230" /></div>

<br />
## 比较检验

to be continued