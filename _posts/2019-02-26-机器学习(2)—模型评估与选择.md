---
layout: post
title:  "机器学习(2)—模型评估与选择"
date:   2019-02-26
desc: ""
keywords: "机器学习"
categories: [机器学习]
tags: [机器学习]
icon: icon-html
---

## 经验误差与过拟合：

学习器在训练集上的误差称为 “ 训练误差 ” 或 “ 经验误差 ” ，在新样本上的误差称为 “ 泛化误差 ”

当学习器把训练样本学得“太好”，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象在机器学习中称为 “ 过拟合 ” （overfitting）

与 “ 过拟合 ” 相对的是 “ 欠拟合 ” ，这是指对训练样本的一般性质尚未学好

通过实验测试来对学习器的泛化误差进行评估并进而做出选择，为此，需要一个 “ 测试集 ” 来测试学习器对新样本的判别能力，然后以测试集上的 “ 测试误差 ” 作为泛化误差的近似

测试集应尽可能与训练集互斥

<br />
## 评估方法：

**留出法 （hold-out）** ：直接将数据集 D 划分为两个互斥的集合，其中一个集合作为训练集 S， 另一个作为测试集 T，即 $$D=S\cup T,S\cap T=\varnothing$$. 在 S 上训练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计

**交叉验证法 （cross validation）** ：先将数据集 D 划分为 k 个大小相似的互斥子集，即

$$D = D_{1}\cup D_{2}\cup ...\cup D_{k},\quad D_{i}\cap D_{j}=\varnothing (i \neq j)$$

每个子集 $$D_{i}$$ 都尽可能保持数据分布的一致性，即从 D 中通过分层采样得到

然后每次用 k - 1 个子集的并集作为训练集，余下的那个子集作为测试集； 这样得到了 k 组训练/测试集，最终返回 k 个测试结果的均值

<img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/6.png" width = "370" height =
"220"/>

通常称为 **k 折交叉验证 k-fold cross validation**

**留一法 （Leave-One-Out）** 交叉验证法中，令 k = m

**自助法 （bootstrapping）** 以自主采样法为基础。给定包含 m 个样本的数据集 D，这样采样产生数据集 $$D^{'}$$ : 每次随机从 D 中挑选一个样本将其拷贝放入 $$D^{'}$$ ，然后再将该样本放回初始数据集 D 中，使得该样本在下次采样时仍有可能被采到。

通过自主采样，D 中约有 36.8% 的样本未出现在 $$D^{'}$$ 中，这样，实际评估的模型与期望评估的模型都使用 m 个训练样本，而仍有数据总量约 1 / 3 的，没在训练集中出现的样本用于测试，这样的测试结果也叫做 “ 包外估计 ” out-of-bag estimate

**参数调节 / 调参** ： 大多数学习算法都有些参数需要设定，参数配置不同，学得模型的性能往往有显著差别

<br />
## 性能度量

性能度量是衡量模型泛化能力的评价标准，什么样的模型是好的，不仅取决于算法和数据，还决定于任务需求

<img src="https://raw.githubusercontent.com/Tianye-Zheng/Tianye-Zheng.github.io/master/PostPictures/2019-02-25/7.png" width = "370" height =
"220"/>